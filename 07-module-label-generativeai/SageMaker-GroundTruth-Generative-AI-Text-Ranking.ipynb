{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337df1d5",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Ground Truth Demonstration for Ranking Model Responses\n",
    "\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "    1. [Create a Work Team](#Create-Workteam)\n",
    "    2. [IAM Role Set up](#IAM-role-set-up)\n",
    "    3. [Prepare the data](#Prepare-the-data)\n",
    "3. [Run a Ground Truth labeling job](#Run-a-Ground-Truth-labeling-job)\n",
    "    1. [Specify Parameters for Labeling Job](#Specify-Labeling-Parameters)\n",
    "    2. [Create the instruction template](#Create-the-instruction-template)\n",
    "    3. [Create a Labeling Job](#Create-a-Labeling-Job)\n",
    "    4. [Gather Human feedback through labeling portal](#Gather-human-feedback-through-labeling-portal)\n",
    "    5. [Monitoring Labeling Job Status](#Monitoring-Labeling-Job-Status)\n",
    "4. [Post Processing and Analysis of SageMaker GroundTruth Labeling Job Results](#Post-Processing-and-Analysis-of-SageMaker-Ground-Truth-Labeling-Job-Results)\n",
    "    1. [Merging Worker Responses](#Merging-Worker-Responses-into-a-Consolidated-JSON-File)\n",
    "    2. [Analyzing Time Spent on Labeling Tasks](#Analyzing-Time-Spent-on-Labeling-Tasks)\n",
    "    3. [Consolidating Worker Responses and Output Manifest](#Consolidating-Worker-Responses-and-Output-Manifest)\n",
    "    4. [Reordering Model Responses Based on Human Evaluation](#Reordering-Model-Responses-Based-on-Human-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594bc8f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook guides you through the process of setting up a Human-in-the-Loop (HITL) workflow for Reinforcement Learning from Human Feedback (RLHF) using Amazon SageMaker Ground Truth's Text Ranking template. The objective is to rank model-generated responses using human annotators. This labeling UI facilitates human annotators to evaluate and rank model responses based on various dimensions such as \"clarity,\" \"bias,\" and \"accuracy.\" The UI presents prompts alongside responses from multiple models or the same model with different parameters, enabling detailed comparative analysis.\n",
    "\n",
    "Some key use cases for this labeling UI include:\n",
    "\n",
    "<b>Model selection:</b> Customers can use the generated rankings to determine which pre-existing model best suits their specific needs and use cases.\n",
    "\n",
    "<b>Fine-tuning guidance:</b> The dimensional rankings provide detailed feedback that can inform the fine-tuning process of custom language models, helping to improve performance in targeted areas.\n",
    "\n",
    "<b>Bias detection and mitigation:</b> By including bias as an evaluation dimension, organizations can identify and address potential biases in model outputs.\n",
    "\n",
    "<b>Competitive analysis:</b> Companies can use this UI to benchmark their models against competitors or industry standards.\n",
    "\n",
    "<b>User preference analysis:</b> By involving end-users in the annotation process, companies can gain insights into user preferences and expectations.\n",
    "\n",
    "<b>Domain-specific optimization:</b> The flexibility to define custom dimensions allows for evaluation tailored to specific domains or industries.\n",
    "\n",
    "<b>Iterative model improvement:</b> Regular use of this UI can facilitate an iterative approach to model development, with each round of feedback informing subsequent improvements.\n",
    "\n",
    "The steps include  setting up necessary Ground Truth pre-requisites, downloading a JSON with prompts and responses, converting it to a Ground Truth input manifest, creating a worker task template, creating and monitoring a labeling job, and post-processing the results to deliver a consolidated dataset with ranked responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c51be",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will create some of the resources you need to launch a Ground Truth labeling job in this notebook.\n",
    "\n",
    "Lets get the latest version of SDK, restart kernel and  import some essential libraries to set up the environment for downloading data, handling JSON files, and leveraging AWS services for machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install awscli -q --upgrade\n",
    "!pip install botocore -q --upgrade\n",
    "!pip install sagemaker -q --upgrade\n",
    "!pip install py7zr\n",
    "!pip install datasets\n",
    "\n",
    "# NOTE: Restart Kernel after the above command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd256839",
   "metadata": {},
   "source": [
    "### Create-Workteam \n",
    "A work team is a group of workers that complete labeling tasks. If you want to preview the worker UI and execute the labeling task you will need to create a private work team, add yourself as a worker to this team, and provide the work team ARN below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKTEAM_ARN = \"\"\n",
    "\n",
    "print(f\"This notebook will use the work team ARN: {WORKTEAM_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0d3f1",
   "metadata": {},
   "source": [
    "### IAM-role-set-up\n",
    "The IAM execution role you used to create this notebook instance must have the following permissions:\n",
    "\n",
    "AWS managed policy AmazonSageMakerGroundTruthExecution. Run the following code-block to see your IAM execution role name. This GIF demonstrates how to add this policy to an IAM role in the IAM console. You can also find instructions in the IAM User Guide: Adding and removing IAM identity permissions.\n",
    "\n",
    "When you create your role, you specify Amazon S3 permissions. Make sure that your IAM role has access to the S3 bucket that you plan to use in this example. If you do not specify an S3 bucket in this notebook, the default bucket in the AWS region you are running this notebook instance will be used. If you do not require granular permissions, you can attach AmazonS3FullAccess to your role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "role_name = role.split(\"/\")[-1]\n",
    "print(\"********************************************************************************\")\n",
    "print(\"The IAM execution role name:\", role_name)\n",
    "print(\"The IAM execution role ARN:\", role)\n",
    "print(\"********************************************************************************\")\n",
    "print(\n",
    "    \"IMPORTANT: Make sure this execution role has the AWS Managed policy AmazonGroundTruthExecution attached.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2878113",
   "metadata": {},
   "source": [
    "### Prepare-the-data\n",
    "Before we create the labeling job, we need to ensure that the input data is in the format expected by GroundTruth. We use the prompts and responses we collected from our model in the <b>input_prompts.json</b> file to create the manifest file <b>gt_input_manifest_textranking.json</b>. Each row in the manifest file contains an object(prompt-response pair). We upload the file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the JSON file\n",
    "with open('data/input_prompts.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 2: Convert to the desired format\n",
    "formatted_data = \"\"\n",
    "for item in data:\n",
    "    entry = {\n",
    "        \"source\": item[\"prompt\"],\n",
    "        \"responses\": item[\"responses\"]\n",
    "    }\n",
    "    formatted_data += json.dumps(entry) + \"\\n\"\n",
    "\n",
    "# Step 3: Save the output as a new JSON file\n",
    "with open('gt_input_manifest_textranking.json', 'w') as f:\n",
    "    f.write(formatted_data)\n",
    "\n",
    "print(\"Conversion complete. The file 'gt_input_manifest_textranking.json' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c34be4",
   "metadata": {},
   "source": [
    "#### Upload the Input manifest into S3 \n",
    "The JSON that is transformed into a Ground Truth input manifest format where each entry contains a \"source\" field with the prompt and \"responses\" field with the model responses, is uploaded to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session and S3 resource\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "# Define the S3 bucket and folder\n",
    "bucket_name = bucket\n",
    "prefix = 'genai'\n",
    "output_file = 'gt_input_manifest_textranking.json'\n",
    "s3_path = f\"{prefix}/{output_file}\"\n",
    "\n",
    "# Upload the file to S3\n",
    "s3.Bucket(bucket_name).upload_file(output_file, s3_path)\n",
    "input_manifest_uri = f\"s3://{bucket_name}/{s3_path}\"\n",
    "\n",
    "print(f\"File uploaded to s3://{bucket_name}/{s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514d8fa",
   "metadata": {},
   "source": [
    "## Run-a-Ground-Truth-labeling-job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2eb876",
   "metadata": {},
   "source": [
    "###  Create-the-instruction-template\n",
    "\n",
    "The instruction template dictates what is be displayed on the UI at the time when the human raters review the prompt and model responses and provide feedback. It contains instructions to help them perform their task accurately.\n",
    "\n",
    "The template below advises human raters to review the Prompt and Model responses and rank the latter based on two dimensions specified under \"ordinal-ranking-dimensions\" - Accuracy and Clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2325c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def make_template(save_fname=\"instructions_tr.template\"):\n",
    "    template = \"\"\"\n",
    "    <html>\n",
    "      <head>\n",
    "        <meta charset=\"UTF-8\" />\n",
    "        <link rel=\"stylesheet\" href=\"https://assets.crowd.aws/css/crowd-html-elements-v2.css\" />\n",
    "        <link rel=\"icon\" href=\"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>&#129351;</text></svg>\" />\n",
    "        <title>Text Ranking Tool</title>\n",
    "        <script src=\"https://assets.crowd.aws/crowd-html-elements-v2s.js\"></script>\n",
    "      </head>\n",
    "\n",
    "      <body>\n",
    "        <crowd-form id=\"crowd-form-submit\">\n",
    "          <crowd-text-ranking\n",
    "            name=\"textRanking\"\n",
    "            ordinal-ranking-dimensions='[{\"name\":\"Accuracy\",\"allowTie\":true},{\"name\":\"Clarity\",\"allowTie\":true}]'\n",
    "            text='{{ task.input.source }}'\n",
    "            responses='{{ task.input.responses | to_json }}' >\n",
    "            <short-instructions>\n",
    "               <p>Rank the following responses from a language model according to the Dimensions on the right panel.</p>\n",
    "            </short-instructions>\n",
    "          </crowd-text-ranking>\n",
    "        </crowd-form>\n",
    "        <script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    with open(save_fname, \"w\") as f:\n",
    "        f.write(template)\n",
    "\n",
    "# Create the template file locally\n",
    "make_template(save_fname=\"./instructions_tr.template\")\n",
    "\n",
    "# Define the S3 path\n",
    "file_name = 'instructions_tr.template'\n",
    "ui_s3_path = f\"{prefix}/{file_name}\"\n",
    "UITEMPLATES3URI = f\"s3://{bucket}/{ui_s3_path}\"\n",
    "\n",
    "# Upload the file to S3 using the s3 client\n",
    "s3_client.upload_file(\"./instructions_tr.template\", bucket, ui_s3_path)\n",
    "\n",
    "print(f\"File uploaded to {UITEMPLATES3URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742c776",
   "metadata": {},
   "source": [
    "### Specify-Labeling-Parameters\n",
    "Specify a Labeling Job Name, Parameters for the Labeling such as TaskTitle, TaskDescription, TaskKeywords and use the CreateLabelingJob API to launch the Ground truth Labeling Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "labeling_job_name = \"immersionday-genai-text-ranking-\" + timestamp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37572f5",
   "metadata": {},
   "source": [
    "### Create-a-Labeling-Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "\n",
    "client.create_labeling_job(\n",
    "    LabelingJobName=labeling_job_name,\n",
    "    LabelAttributeName='label',\n",
    "    InputConfig={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'ManifestS3Uri': input_manifest_uri #Enter S3 URI of Input Data Json\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputPath': f's3://{bucket}/output/' #Enter S3 URI of Output folder\n",
    "    },\n",
    "    RoleArn=role, #Enter IAM Sagemaker Execution Role here,\n",
    "    HumanTaskConfig={\n",
    "        'WorkteamArn': WORKTEAM_ARN, #Enter Workteam ARN\n",
    "        'UiConfig': {\n",
    "            'UiTemplateS3Uri': UITEMPLATES3URI #Enter S3 URI of UI template\n",
    "        },\n",
    "        'TaskKeywords': [\n",
    "            'QnA',\n",
    "        ],\n",
    "        'TaskTitle': 'Generative AI - Text Ranking',\n",
    "        'TaskDescription': \"Rank the responses provided by the Models based on the Dimensions\",\n",
    "        'NumberOfHumanWorkersPerDataObject': 1,\n",
    "        'TaskTimeLimitInSeconds': 60*30,\n",
    "        'TaskAvailabilityLifetimeInSeconds': 60*60*24*10,\n",
    "        'MaxConcurrentTaskCount': 100\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48480676",
   "metadata": {},
   "source": [
    "### Gather-human-feedback-through-labeling-portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "workforce = client.describe_workforce(WorkforceName=\"default\")\n",
    "worker_portal_url = 'https://' + workforce[\"Workforce\"][\"SubDomain\"]\n",
    "\n",
    "\n",
    "# Display the URL and instructions\n",
    "display(HTML(f\"\"\"\n",
    "<body>\n",
    "<h4>Gather human preference data</h4>\n",
    "<p>Please complete the human evaluation tasks available in the labeling portal.</p>\n",
    "<p><a href=\"{worker_portal_url}\">{worker_portal_url}</a>\n",
    "<p><b>Ensure all tasks are completed before proceeding to the next steps in this notebook.<b></p>\n",
    "<body>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d18941",
   "metadata": {},
   "source": [
    "### Monitoring-Labeling-Job-Status\n",
    "We track the status of the ongoing labeling job. It is essential to monitor the job's progress and wait for its completion by the annotators. Once the labeling job is finished, we can then proceed to gather feedback from the annotators. This process ensures that we only collect feedback after the entire job is completed, thereby maintaining the accuracy and reliability of the feedback collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e37f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.describe_labeling_job(LabelingJobName=labeling_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea1df2",
   "metadata": {},
   "source": [
    "## Post-Processing-and-Analysis-of-SageMaker-Ground-Truth-Labeling-Job-Results\n",
    "<b>[OPTIONAL]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6abfb",
   "metadata": {},
   "source": [
    "### Merging-Worker-Responses-into-a-Consolidated-JSON-File\n",
    "\n",
    "This section focuses on combining all individual worker responses into a single, comprehensive JSON file. We gather the individual JSON files from the \"worker-response\" folder, each representing a worker's answers to labeling tasks, and merge them into one consolidated file. The resulting consolidated file makes it easier to analyze worker performance, compare responses across different tasks, and prepare the data for further processing or quality checks.\n",
    "\n",
    "In the output file, each set of responses is accompanied by an index (e.g., [2,1,3]) that represents how human annotators reordered the original responses. For instance, [2,1,3] means the second response was ranked first, the first response second, and the third response remained in its original position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def merge_json_files(bucket, prefix, output_bucket, output_key):\n",
    "    merged_data = []\n",
    "\n",
    "    # List iteration buckets\n",
    "    iteration_response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/')\n",
    "    for iteration in iteration_response.get('CommonPrefixes', []):\n",
    "        # List JSON objects in each iteration bucket\n",
    "        json_response = s3_client.list_objects_v2(Bucket=bucket, Prefix=iteration['Prefix'])\n",
    "        for obj in json_response.get('Contents', []):\n",
    "            # Read and append each JSON file, adding the S3 URI or file name\n",
    "            key = obj['Key']\n",
    "            json_obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "            json_data = json.loads(json_obj['Body'].read().decode('utf-8'))\n",
    "            json_data['workerjsonUri'] = f\"s3://{bucket}/{key}\"  # Add the source S3 URI to the JSON object\n",
    "            merged_data.append(json_data)  # Append the modified JSON object\n",
    "\n",
    "    # Write merged data to a new JSON file in S3\n",
    "    merged_json = json.dumps(merged_data)\n",
    "    s3_client.put_object(Body=merged_json, Bucket=output_bucket, Key=output_key)\n",
    "    print(f\"Merged JSON file created at https://s3.console.aws.amazon.com/s3/object/{output_bucket}?region={region}&prefix={output_key}\")\n",
    "\n",
    "# Replace with your actual bucket names and prefixes\n",
    "source_bucket_name = bucket\n",
    "iteration_prefix = f'output/{labeling_job_name}/annotations/worker-response/iteration-1/'\n",
    "output_bucket_name = bucket\n",
    "output_key = f'output/{labeling_job_name}/merged-worker-data.json'\n",
    "\n",
    "merge_json_files(source_bucket_name, iteration_prefix, output_bucket_name, output_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a187c7f",
   "metadata": {},
   "source": [
    "### Analyzing-Time-Spent-on-Labeling-Tasks\n",
    "This section analyzes the time spent on labeling tasks in our SageMaker Ground Truth job, from the consolidated worker json file. It calculates the total time invested across all tasks, counts the number of completed tasks, and determines the average time per task. These insights help us understand the overall effort of our labeling project, assess its scale, and plan future workflows more effectively. By examining these time metrics, we can make informed decisions about resource allocation and set realistic expectations for upcoming labeling jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Back, Style, init\n",
    "\n",
    "init(autoreset=True)  # Initialize colorama\n",
    "\n",
    "def calculate_and_display_average_time(bucket, key):\n",
    "    json_obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    merged_json_data = json.loads(json_obj['Body'].read().decode('utf-8'))\n",
    "    total_time = 0\n",
    "    count = 0\n",
    "    for json_data in merged_json_data:\n",
    "        if 'answers' in json_data:\n",
    "            for answer in json_data['answers']:\n",
    "                if 'timeSpentInSeconds' in answer:\n",
    "                    total_time += answer['timeSpentInSeconds']\n",
    "                    count += 1\n",
    "    \n",
    "    average_time = total_time / count if count > 0 else 0\n",
    "    \n",
    "    # Create a fancy display\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(Fore.CYAN + Style.BRIGHT + \"     LABELING TASK SUMMARY     \")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(Fore.YELLOW + \"📊 \" + Style.BRIGHT + \"Count of Tasks:\")\n",
    "    print(Fore.WHITE + f\"   {count:,} tasks\")\n",
    "    \n",
    "    print(Fore.YELLOW + \"\\n⏱️ \" + Style.BRIGHT + \"Total Time Spent:\")\n",
    "    print(Fore.WHITE + f\"   {total_time:.2f} seconds\")\n",
    "    print(Fore.WHITE + f\"   ({total_time / 3600:.2f} hours)\")\n",
    "    \n",
    "    print(Fore.YELLOW + \"\\n⌛ \" + Style.BRIGHT + \"Average Time per Task:\")\n",
    "    print(Fore.WHITE + f\"   {average_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "calculate_and_display_average_time(output_bucket_name, output_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c55aa5",
   "metadata": {},
   "source": [
    "### Consolidating-Worker-Responses-and-Output-Manifest\n",
    "\n",
    "This section consolidates our labeling project data into a comprehensive single file. We merge information from two key sources: the worker responses and the output manifest. By combining the \"source\" data from the output manifest with the \"answers\" provided by workers, we create a unified view of all labeling tasks. This consolidation streamlines our data analysis process, allowing us to easily connect each task's original source with its corresponding worker annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_with_manifest(output_manifest_bucket, output_manifest_key, merged_data_bucket, merged_data_key):\n",
    "    try:\n",
    "        # Fetch and load the output manifest file\n",
    "        manifest_obj = s3_client.get_object(Bucket=output_manifest_bucket, Key=output_manifest_key)\n",
    "        manifest_data = manifest_obj['Body'].read().decode('utf-8').splitlines()\n",
    "        \n",
    "        # Load the merged JSON data\n",
    "        merged_data_obj = s3_client.get_object(Bucket=merged_data_bucket, Key=merged_data_key)\n",
    "        merged_data = json.loads(merged_data_obj['Body'].read().decode('utf-8'))\n",
    "        \n",
    "        # Convert merged data to a dictionary for easier lookup based on workerjsonUri\n",
    "        merged_data_dict = {item['workerjsonUri']: item for item in merged_data}\n",
    "        \n",
    "        final_data = []\n",
    "        \n",
    "        for line in manifest_data:\n",
    "            manifest_entry = json.loads(line)\n",
    "            worker_response_ref = manifest_entry.get(\"label-metadata\", {}).get(\"worker-response-ref\")\n",
    "            \n",
    "            # Find the matching entry in the merged data\n",
    "            if worker_response_ref in merged_data_dict:\n",
    "                # Merge the manifest data with the corresponding worker response data\n",
    "                combined_data = {**manifest_entry, **merged_data_dict[worker_response_ref]}\n",
    "                final_data.append(combined_data)\n",
    "        \n",
    "        # Optionally, write the final merged data to a new file in S3 or handle it as needed\n",
    "        final_json = json.dumps(final_data)\n",
    "        final_output_key = f'output/{labeling_job_name}/final-merged-data.json'  # Customize this as needed\n",
    "        s3_client.put_object(Body=final_json, Bucket=output_manifest_bucket, Key=final_output_key)\n",
    "        print(f\"Final merged JSON file created at https://s3.console.aws.amazon.com/s3/object/{output_manifest_bucket}?region={region}&prefix={final_output_key}\")\n",
    "    \n",
    "    except s3_client.exceptions.NoSuchKey as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Replace with your actual bucket names and keys\n",
    "output_manifest_bucket = bucket\n",
    "output_manifest_key = f'output/{labeling_job_name}/manifests/output/output.manifest'\n",
    "merged_data_bucket = bucket\n",
    "merged_data_key = f'output/{labeling_job_name}/merged-worker-data.json'\n",
    "\n",
    "merge_data_with_manifest(output_manifest_bucket, output_manifest_key, merged_data_bucket, merged_data_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d55362",
   "metadata": {},
   "source": [
    "### Reordering-Model-Responses-Based-on-Human-Evaluation\n",
    "In the output file, each set of responses is accompanied by an index (e.g., [2,1,3]) that represents how human annotators reordered the original responses. The code here matches these index values with the actual responses from our consolidated file. It then reorganizes and displays the full text of these responses in the order determined by the human evaluators. This process allows us to see how human judgment has altered the ranking of model outputs, providing insights into which responses were deemed more appropriate or higher quality by our annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 bucket and key for the input JSON\n",
    "bucket_name = bucket\n",
    "input_key = f'output/{labeling_job_name}/final-merged-data.json'\n",
    "output_key = f'output/{labeling_job_name}/reordered_responses.json'\n",
    "\n",
    "# Function to replace rankings with response text, handling tie rankings\n",
    "def replace_rankings_with_responses(data):\n",
    "    for item in data:\n",
    "        responses = item[\"responses\"]\n",
    "        for answer in item[\"answers\"]:\n",
    "            for dimension in answer[\"answerContent\"][\"ordinalRankingDimensions\"]:\n",
    "                rankings = dimension[\"responseRankings\"]\n",
    "                # Store the original rankings in a new field\n",
    "                dimension[\"responseIndex\"] = rankings\n",
    "                # Create a list to store the response texts based on rankings\n",
    "                ranked_responses = []\n",
    "                for rank in sorted(set(rankings)):\n",
    "                    # Add all responses that match the current rank\n",
    "                    ranked_responses.extend([responses[i - 1] for i, r in enumerate(rankings, start=1) if r == rank])\n",
    "                dimension[\"responseRankings\"] = ranked_responses\n",
    "    return data\n",
    "\n",
    "# Load the JSON data from S3\n",
    "def load_json_from_s3(bucket, key):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    data = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "    return data\n",
    "\n",
    "# Save the JSON data to S3\n",
    "def save_json_to_s3(data, bucket, key):\n",
    "    json_data = json.dumps(data, indent=2)\n",
    "    s3_client.put_object(Body=json_data, Bucket=bucket, Key=key)\n",
    "\n",
    "# Main processing\n",
    "def main():\n",
    "    # Load data from S3\n",
    "    data = load_json_from_s3(bucket_name, input_key)\n",
    "    \n",
    "    # Replace rankings with responses\n",
    "    updated_data = replace_rankings_with_responses(data)\n",
    "    \n",
    "    # Save updated data to S3\n",
    "    save_json_to_s3(updated_data, bucket_name, output_key)\n",
    "    \n",
    "    print(f\"Updated JSON data has been saved to https://s3.console.aws.amazon.com/s3/object/{bucket_name}?region={region}&prefix={output_key}\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
